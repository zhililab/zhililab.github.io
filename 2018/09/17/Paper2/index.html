<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><title>论文：基于 Co-occurrence 的词库自动生成研究 | WELCOME</title><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name="format-detection" content="telephone=no"><meta name="author" content="[object Object]"><meta name="designer" content="minfive"><meta name="keywords" content="威胁情报，网络安全，数据分析, python, r, sql, mongodb"><meta name="description" content="介绍：日常学习与技术交流的个人博客"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=yes"><meta name="mobile-web-app-capable" content="yes"><meta name="robots" content="all"><link rel="canonical" href="https://github.com/zhililab/2018/09/17/Paper2/index.html"><link rel="icon" type="image/png" href="/assets/images/favicon/favicon.ico" sizes="32x32"><link rel="stylesheet" href="/scss/base/index.css"><link rel="alternate" href="/atom.xml" title="ZHILI"><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e3267498201dfa9699a5c509424709d6";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-108468870-1"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-108468870-1")</script><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet"><style>.pace .pace-progress{background:#fff;height:3px}.pace .pace-progress-inner{box-shadow:0 0 10px #fff,0 0 5px #fff}.pace .pace-activity{border-top-color:#fff;border-left-color:#fff}</style><link rel="stylesheet" href="/scss/views/page/post.css"></head><body ontouchstart><div id="page-loading" class="page page-loading" style="background-image:url(http://oo12ugek5.bkt.clouddn.com/blog/images/loader.gif)"></div><div id="page" class="page js-hidden"><header class="page__small-header page__header--small"><nav class="page__navbar"><div class="page__container navbar-container"><a class="page__logo" href="/" title="ZHILI" alt="ZHILI"><img src="/assets/images/logo/LogoMakr_0xKVom.png" alt="ZHILI"></a><nav class="page__nav"><ul class="nav__list clearfix"><li class="nav__item"><a href="/" alt="首页" title="首页">首页</a></li><li class="nav__item"><a href="/archives" alt="归档" title="归档">归档</a></li><li class="nav__item"><a href="/notes" alt="札记" title="札记">札记</a></li><li class="nav__item"><a href="/about" alt="关于" title="关于">关于</a></li></ul></nav><button class="page__menu-btn" type="button"><i class="iconfont icon-menu"></i></button></div></nav></header><main class="page__container page__main"><div class="page__content"><article class="page__post"><div class="post__cover"><img src="/assets/images/cover/Paper_cover.png" alt="论文：基于 Co-occurrence 的词库自动生成研究"></div><header class="post__info"><h1 class="post__title">论文：基于 Co-occurrence 的词库自动生成研究</h1><div class="post__mark"><div class="mark__block"><i class="mark__icon iconfont icon-write"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="https://www.github.com/zhililab">Zhi Li</a></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-time"></i><ul class="mark__list clearfix"><li class="mark__item"><span>2018-09-17</span></li></ul></div><div class="mark__block"><i class="mark__icon iconfont icon-tab"></i><ul class="mark__list clearfix"><li class="mark__item"><a href="/tags/词库自动构建/">词库自动构建</a></li><li class="mark__item"><a href="/tags/词汇共现/">词汇共现</a></li></ul></div></div></header><div class="post__content"><p>本文翻译自《Automatic Thesaurus Generation using Co-occurrence》</p><p>原文链接：<a href="https://pdfs.semanticscholar.org/66d1/768083e11c0605fe206c67f62a469caab902.pdf" target="_blank" rel="noopener">link</a></p><p><img src="https://user-images.githubusercontent.com/11768073/45431952-3b131700-b6db-11e8-806f-d9a65fd6a2df.png" alt="image"></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>本文提出了通过与词表共现的信息性来表征有用的词库术语</strong>。 给定文档语料库，信息性被正式化为包含该术语的所有文档的加权平均期限分布的信息增益。 虽然得到的词库生成算法是 <strong>无监督的</strong> ，但我们发现 <strong>高信息性术语对应于大而连贯的文档子集</strong> 。 我们通过比较高信息性词汇与文章的维基百科类别的关键词来评估我们在一组荷兰维基百科文章中的方法。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>我们考虑使用统计方法为给定集合生成同义词库的问题。 该问题与从关键字列表中的文本分配关键字以及为语料库的给定子集找到最具特征性的术语相关，但有所不同。我们的方法是制作一个术语列表，这些术语对于理解整个集合最具信息性。当前方法的部分吸引力在于它提出了一种统计模型来形式化（好）关键词的概念。 如果我们相信导致这个模型的假设，那么生成词库的高级算法几乎被强加给我们。</p><p><strong>我们的主要假设是术语的共现是其含义的代理</strong>[11,14,9]。为了使用这些信息，我们计算每个术语所有合作术语的分布。然后，我们可以将此共生术语分布用作集合上下文中术语含义的代理，并将其与单个文档的术语分布进行比较。如果文档的术语分布类似于其共同出现的术语的分布，则我们假设文档在语义上与术语相关。幸运的是，概率分布，相对熵或 Kullback-Leibler 散度之间存在着天然相似性度量。如果我们遵循这种形式化，那么有一个明显的策略可以生成一个词库，作为一组术语，它们可以获得最大的信息增益，定义为 Kullback-Leibler 差异的差异。在实践中，这个模型是一个轻微简化的版本，这里比如是因为关键词可以用不同的术语来表征。我们将在第5节讨论这个问题。</p><p>本文的组织如下，在第2节中我们讨论了一些相关的工作。 在第3节中，我们介绍了不同的概率分布和 Kullback-Leibler 散度的信息理论概念，这些概念将成为本文其余部分的基础。 我们在第4节中使用这些来给出可用于对关键字进行排名的各种信息增益定义。在第5节中，我们评估了关于荷兰语维基百科的语料库的概念。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>找到用于描述集合的文档的词库术语集的问题与确定索引文档的术语权重的问题密切相关。特别地，它涉及这样的方法，其中文档的术语的权重是根据其在文档中的频率及其在整个集合中的重要性来计算的。最优雅的方法之一是Salton 的术语歧视模型（见 [15,18]，以及那里引用的参考文献）。给定向量空间模型，其中每个维度表示一个术语，每个文档可以表示为向量。如果从模型中删除相应的维度，则术语的区分值是所有文档之间的平均距离的变化。当然，可以使用各种距离测量来计算该模型中的文档之间的距离。从概念上讲，这种方法与我们的方法有些类似：删除具有高识别值的术语会导致更高的密度或更低的熵。但是，我们不会考虑删除术语的效果，而是查看通过了解术语包含在文档中可以获得的压缩。<strong>因此，术语的共现在我们的方法中起着至关重要的作用，而在计算歧视值时没有考虑到这一点</strong>。</p><p>文献[3]追求解决一组词库术语问题的另一种方法。 他们开始聚类文档（基于术语向量空间模型中的距离），然后尝试查找聚类的判别项。</p><p>本文中使用的方法有点类似于潜在语义分析[5,9]，因为它们都以术语和文档的共现矩阵为出发点。虽然当前的方法也会导致关键字的组合，但它具有仅导致关键字的概率分布（即，具有总权重 1 的关键字的加权和）而不是正和负组合的主要优点。它还基于概念上相似但技术上不同的关键词之间的接近概念，其具有更直接的信息理论解释。不幸的是，就像潜在语义分析所需的奇异值分解一样，它在计算上很重。我们的方法更接近[6,7]的概率潜在语义分析（PLSA）并受其影响。与 PLSA 一样，我们的方法基于<strong>最大化 Kullback-Leibler 散度和最大似然</strong>。然而，与霍夫曼的工作不同，我们试图找出概率分布而不是文本的某些假定的基本抽象方面，这些方面“解释”文档中观察到的术语分布。</p><p>关于将关键字分配给单个文档或文档语料库的子集，还做了很多工作。 作为这一研究领域的例子，我们参考[1,8]和[17]，他们专注于在生物医学文章的不同部分找到关键词。</p><p>关于本文的信息理论背景，有大量文献。 我们使用[2]作为我们的主要信息理论参考，我们发现了一个非常易读的介绍。</p><h2 id="3-术语和文档分布"><a href="#3-术语和文档分布" class="headerlink" title="3 术语和文档分布"></a>3 术语和文档分布</h2><p>我们将文档简化为一个单词或术语。一旦我们接受这个模型，所有不同文档中所有不同术语的出现次数是我们留下的唯一信息。因此，考虑一组 n 个术语出现 W ，每个术语 W 是 T = {t 1，… tm}中的术语 t 的实例，并且每个出现在集合 <img src="https://user-images.githubusercontent.com/11768073/45591898-61aea780-b992-11e8-8a1c-cb8ec1520ffe.png" alt="image"> 。设 <img src="https://user-images.githubusercontent.com/11768073/45591900-725f1d80-b992-11e8-99dd-1a7beb747b72.png" alt="image"> 为 d 中项 t 的出现次数，<img src="https://user-images.githubusercontent.com/11768073/45591895-53f92200-b992-11e8-816e-127f33670310.png" alt="image">为项t的出现次数，<img src="https://user-images.githubusercontent.com/11768073/45591891-40e65200-b992-11e8-9bfc-6c24029ba66a.png" alt="image"> 中的术语出现次数。</p><h3 id="3-1-基本分布"><a href="#3-1-基本分布" class="headerlink" title="3.1 基本分布"></a>3.1 基本分布</h3><p>我们定义C×T上的概率分布Q，C上的分布Q和T上的Q，其测量随机选择术语出现的概率，以及相应的术语或源文档。</p><p><img src="https://user-images.githubusercontent.com/11768073/45469139-da2b2380-b75a-11e8-8d2b-706a27139fd3.png" alt="image"></p><p>这些分布是我们将在余数中做的所有事情的基线概率分布，我们将在确定C和W的子集的“大小”时使用它们来支持简单的计数测量。此外，我们有两个重要的条件概率分布。</p><p><img src="https://user-images.githubusercontent.com/11768073/45469169-fc24a600-b75a-11e8-9bd2-c76020873c7f.png" alt="image"></p><p>我们使用符号Q（d | t）作为 t 的源分布，因为它是随机出现的项t具有源d的概率。 类似地，q（t | d），d的项分布是文档d中的随机项出现是项t的实例的概率。 C×T，C和T上的其他概率分布将由具有各种子和上标的P，P，p表示。</p><h3 id="3-2-贡献词汇分布"><a href="#3-2-贡献词汇分布" class="headerlink" title="3.2 贡献词汇分布"></a>3.2 贡献词汇分布</h3><p>上一节中的设置允许我们在文档和术语集上设置马尔可夫链，这将允许我们将概率分布从术语传播到文档，反之亦然。 考虑具有转移 C→T 的 T∪C 上的马尔可夫链，转移概率Q（d | t）和转移T→C仅具有转移概率q（t | d）。</p><p>给定项分布 p（t），我们计算一步马尔可夫链演化。 这给了我们一个文档分布P p（d），在特定文档中找到一个术语出现的概率，假设出现的术语分布是 p</p><p><img src="https://user-images.githubusercontent.com/11768073/45469222-3726d980-b75b-11e8-8258-e2ad2cc93b3e.png" alt="image"></p><p>同样，给定文档分布P（d），一步马尔可夫链演化是项分布 <img src="https://user-images.githubusercontent.com/11768073/45469260-66d5e180-b75b-11e8-82e1-f54c3ed66447.png" alt="image">。 由于P（d）给出了在文档d中找到术语出现的概率，因此p P是文档中术语分布的P加权平均值。 结合这些，即两次运行马尔可夫链，每个术语分布产生新的分布</p><p><img src="https://user-images.githubusercontent.com/11768073/45469277-79501b00-b75b-11e8-967d-3b5e7fadaa2a.png" alt="image"></p><p>特别是从简并的“已知为z”项开始，<img src="https://user-images.githubusercontent.com/11768073/45469392-e794dd80-b75b-11e8-9316-fee14a77e3df.png" alt="image">（如果t = z则为1，否则为0），我们得到共生项的分布 <img src="https://user-images.githubusercontent.com/11768073/45469335-a270ab80-b75b-11e8-9b22-3a1ddfb560cb.png" alt="image"></p><p><img src="https://user-images.githubusercontent.com/11768073/45469346-ac92aa00-b75b-11e8-96a7-a21a55485a42.png" alt="image"></p><p>该分布是包含z的文档的术语分布的加权平均，其中权重是在d中出现的术语是z的出现的概率Q（d | z）。</p><p>注意，概率测量<img src="https://user-images.githubusercontent.com/11768073/45469335-a270ab80-b75b-11e8-9b22-3a1ddfb560cb.png" alt="image">与[10，第3节]中的设置非常相似。 但是，我们会跟踪文档中关键字的密度，而不仅仅是文档中关键字的出现或不出现。 这种差异对于长期文档特别相关，其中术语以低密度出现，因为它对平均词分布具有相对高的贡献。 不幸的是，<img src="https://user-images.githubusercontent.com/11768073/45469335-a270ab80-b75b-11e8-9b22-3a1ddfb560cb.png" alt="image">计算起来很昂贵。</p><h2 id="4-关键词的信息性"><a href="#4-关键词的信息性" class="headerlink" title="4 关键词的信息性"></a>4 关键词的信息性</h2><p>直观地，关键字使得在给出类似文档的一些知识的情况下更容易记住文档的内容。 将这种直觉形式化，我们基于Kullback-Leibler 散度（KL-散度）的关键词的信息性基于与给定关键词共同的术语的平均分布与整个集合的术语分布。</p><p>为了方便读者，我们回顾了 KL-divergence 的定义，基本属性和解释[2，sec 2.3]。 给定一组有限的符号X = {x1，x2，…，x n}，其中两个概率分布为 <img src="https://user-images.githubusercontent.com/11768073/45470698-51fc4c80-b761-11e8-8bc7-3f5e2a4e649b.png" alt="image"> 和 <img src="https://user-images.githubusercontent.com/11768073/45470713-5e80a500-b761-11e8-9e27-fb1a50f39800.png" alt="image">，KL-divergence 定义为</p><p><img src="https://user-images.githubusercontent.com/11768073/45470728-6a6c6700-b761-11e8-8361-22859e1c91f6.png" alt="image"></p><p>对于所有 i，很容易证明 <img src="https://user-images.githubusercontent.com/11768073/45470762-8f60da00-b761-11e8-88dc-3ccaaf99d34c.png" alt="image"> 且等于 iff <img src="https://user-images.githubusercontent.com/11768073/45470753-84a64500-b761-11e8-876b-a989c0879017.png" alt="image">。</p><p>KL-divergence 具有以下标准解释。在最佳压缩方案中，根据概率分布p分布的字母X上的符号流 <img src="https://user-images.githubusercontent.com/11768073/45470864-d8189300-b761-11e8-9dea-990d8106c847.png" alt="image"> 至少使用符号 xi 的<img src="https://user-images.githubusercontent.com/11768073/45470889-ec5c9000-b761-11e8-880b-c45df49e8dc7.png" alt="image">位。因此，通过最佳压缩，我们每个符号平均需要 <img src="https://user-images.githubusercontent.com/11768073/45471053-7573c700-b762-11e8-8643-f21117f43140.png" alt="image"> 比特。如果使用适合于分布q的方案来压缩流，我们每个符号平均需要 <img src="https://user-images.githubusercontent.com/11768073/45471321-5295e280-b763-11e8-846a-3b05e6aadceb.png" alt="image"> 比特。因此，KL-散度是通过使用流的实际概率分布p而不是某些先验分布q，每个符号节省的平均比特数。</p><h3 id="4-1-一篇文档的信息增益"><a href="#4-1-一篇文档的信息增益" class="headerlink" title="4.1 一篇文档的信息增益"></a>4.1 一篇文档的信息增益</h3><p>给定一组文件和一个术语，有一个子集合，该术语是相关的。 为了确定这个子集合，我们将定义一个信息理论度量，定义为通过使用专用压缩方案为子集合节省的平均位数，而不是使用整个集合的单个方案。</p><p>将文档 d 的术语分布 p 使用的净信息增益定义为</p><p><img src="https://user-images.githubusercontent.com/11768073/45474390-3d718180-b76c-11e8-9275-06af9cb431a2.png" alt="image"></p><p>它测量从使用p获得的压缩的增益或损失与语料库术语分布q相比较。我们将这称为使用 p 的特定网络信息增益，以强调这是每个单词的增益。显然 <img src="https://user-images.githubusercontent.com/11768073/45474519-95a88380-b76c-11e8-8d06-0a2b426391e5.png" alt="image"> ，并且 <img src="https://user-images.githubusercontent.com/11768073/45474538-a1944580-b76c-11e8-9842-c1796fe8104d.png" alt="image"> 达到唯一最大值<img src="https://user-images.githubusercontent.com/11768073/45591858-87877c80-b991-11e8-8784-3a3095c3340d.png" alt="image"> ，但<img src="https://user-images.githubusercontent.com/11768073/45474593-b96bc980-b76c-11e8-80c3-eebc4d3bd9eb.png" alt="image">是有限的 ，当且仅当 <img src="https://user-images.githubusercontent.com/11768073/45474689-f5069380-b76c-11e8-882a-67debe94c7c2.png" alt="image"> 意味着 <img src="https://user-images.githubusercontent.com/11768073/45474616-c8527c00-b76c-11e8-837d-a0385726beda.png" alt="image">，因为 <img src="https://user-images.githubusercontent.com/11768073/45474663-e7e9a480-b76c-11e8-910a-a0877cb3f2b3.png" alt="image"> 肯定意味着 <img src="https://user-images.githubusercontent.com/11768073/45474616-c8527c00-b76c-11e8-837d-a0385726beda.png" alt="image">。</p><h3 id="4-2-一个子集和的信息增益"><a href="#4-2-一个子集和的信息增益" class="headerlink" title="4.2 一个子集和的信息增益"></a>4.2 一个子集和的信息增益</h3><p>对于文件 D ⊂ C 的子集合，我们通过权衡特定信息增益和文档相对于集合的大小来定义分数净特定信息增益</p><p><img src="https://user-images.githubusercontent.com/11768073/45475369-c12c6d80-b76e-11e8-9e19-e48af56303c0.png" alt="image"></p><p>在子集合D中的文档中定义了D，<img src="https://user-images.githubusercontent.com/11768073/45485413-204bab80-b78a-11e8-98bb-2b0cb8b77e79.png" alt="image"> 和平均时间分布 <img src="https://user-images.githubusercontent.com/11768073/45485423-2b064080-b78a-11e8-87bf-9d80b415240a.png" alt="image"> 的分数，我们将<img src="https://user-images.githubusercontent.com/11768073/45485438-38bbc600-b78a-11e8-91ce-6cbd1c4872d8.png" alt="image"> 重写为</p><p><img src="https://user-images.githubusercontent.com/11768073/45485454-46714b80-b78a-11e8-8112-adfe0698dc85.png" alt="image"></p><p>通常，我们让术语分布 p 取决于所选择的子集 D .对 p 的最明显的选择是 <img src="https://user-images.githubusercontent.com/11768073/45485471-5d17a280-b78a-11e8-84fe-b7155f28b757.png" alt="image"> 显然，<img src="https://user-images.githubusercontent.com/11768073/45485471-5d17a280-b78a-11e8-84fe-b7155f28b757.png" alt="image"> 使 <img src="https://user-images.githubusercontent.com/11768073/45485500-728ccc80-b78a-11e8-9222-53d606e0a6bf.png" alt="image"> 最大化，并且对于我们所拥有的子集合的分数特定增益</p><p><img src="https://user-images.githubusercontent.com/11768073/45485517-7f112500-b78a-11e8-86c4-0229f20b80d4.png" alt="image"></p><h3 id="4-3-一个关键词的信息增益"><a href="#4-3-一个关键词的信息增益" class="headerlink" title="4.3 一个关键词的信息增益"></a>4.3 一个关键词的信息增益</h3><p>考虑受益于压缩的文档的子集 Cz 与针对与 z 共同出现的项的平均分布 <img src="https://user-images.githubusercontent.com/11768073/45484804-f42f2b00-b787-11e8-9eab-806e86b04e0a.png" alt="image">，即 <img src="https://user-images.githubusercontent.com/11768073/45484821-014c1a00-b788-11e8-8c2a-ab1752104f5a.png" alt="image">。然后，我们对关键术语的信息量进行了自然测量。我们定义 <img src="https://user-images.githubusercontent.com/11768073/45484853-2d679b00-b788-11e8-88a4-b066352404fe.png" alt="image">，关键项z的分数净特定信息增益为非负数</p><p><img src="https://user-images.githubusercontent.com/11768073/45484869-3a848a00-b788-11e8-8fd7-4ef00b817808.png" alt="image"></p><p>请注意，即使d中没有出现z，文档d也可能在 Cz 中。 <img src="https://user-images.githubusercontent.com/11768073/45485075-dd3d0880-b788-11e8-86b8-17fae1fb0a07.png" alt="image"> 也是可能的，因为它不必是 <img src="https://user-images.githubusercontent.com/11768073/45485097-f04fd880-b788-11e8-8629-bf5f44c00f61.png" alt="image"> 的情况。 事实上，<img src="https://user-images.githubusercontent.com/11768073/45485113-fc3b9a80-b788-11e8-8881-2f85d17bb813.png" alt="image"> 考虑了文档中出现的 z 的出现部分，而不仅仅是z的出现或不出现，因此可以看作是文档集合的“加权”版本含有 z 。</p><h2 id="5-评估"><a href="#5-评估" class="headerlink" title="5 评估"></a>5 评估</h2><p>我们已经实现了许多技术来从一组文本中提取同义词库。 所有测试的算法都给出了所有检测项的排名。</p><p>为了测试和比较不同的策略，我们编写了一个由758个文档组成的荷兰维基百科文章的小型语料库。在分析阶段，发现了118099个术语，以及26373个唯一术语。这些文章取自8个维基百科类别：航天，绘画，建筑，树木，单子叶植物，航空，流行音乐，鸻形目。选择类别用于主观相似性，如航天和航空，以及流行音乐和单子叶植物等主观相似性。文章在类别中平均分配，但某些类别的文章比其他文章长得多。此外，文章的同质性和特异性在不同类别之间存在显着差异。这清楚地反映在通过将文章中的集合从一个类别和其余类别中分离并使用第4.2节中的公式（1）获得的信息增益中。结果在表1中给出。流行音乐类别得分高的一个原因是，关于该主题的荷兰文章包含许多英文歌曲标题列表，而在其他类别中，英语单词相当罕见。</p><p><img src="https://user-images.githubusercontent.com/11768073/45485265-84ba3b00-b789-11e8-9f17-a7c4e955d104.png" alt="image"></p><h3 id="5-1-预处理"><a href="#5-1-预处理" class="headerlink" title="5.1 预处理"></a>5.1 预处理</h3><p>在提取词库术语之前，我们使用 GATE -framework [4]进行一些预处理。主要的分析步骤是：词形还原，多词查找和命名实体识别。使用 Treetagger [16]完成词形还原和标记。标记允许我们区分内容词和功能词。我们只使用内容词。在词形还原之后，动词，名词和形容词的所有变形形式被映射到它们的词汇形式，从而大大减少了输入数据的变化。由于形容词很少被用作关键词，我们在所有实验中都过滤掉了结果列表中的形容词，即使形容词是相关语义信息的承载者，例如：用化学过程这样的短语。对于多字词查找，我们使用荷兰语维基百科的文章标题，因为假设每个标题代表一个概念是合理的。最后，应用一些简单的规则来识别专有名称。虽然某些组件依赖于语言，但GATE -framework 中的所有组件都可用于多种语言</p><p>作为额外的过滤器，我们要求潜在的同义词词语z必须足够具体。 事实证明，有一些高频率的单词具有较高的 <img src="https://user-images.githubusercontent.com/11768073/45485672-06f72f00-b78b-11e8-8ecd-c3d91b613768.png" alt="image"> ，但不能与任何主题相关联。 因此，我们要求 <img src="https://user-images.githubusercontent.com/11768073/45485777-635a4e80-b78b-11e8-890d-fdea659144e3.png" alt="image"> 与背景分布 q 足够不同，如 Kullback-Leibler 散度所测量的。 我们使用了截断 <img src="https://user-images.githubusercontent.com/11768073/45485685-17a7a500-b78b-11e8-8c03-f706edea3d73.png" alt="image"> 位，结果证明了结果不错。</p><h3 id="5-2-词库构建"><a href="#5-2-词库构建" class="headerlink" title="5.2 词库构建"></a>5.2 词库构建</h3><p>现在，根据 <img src="https://user-images.githubusercontent.com/11768073/45485306-ae736200-b789-11e8-9c81-9b95bee7d962.png" alt="image"> 的值对结果列表进行排序。事实上，我们通过稍微修改 <img src="https://user-images.githubusercontent.com/11768073/45485306-ae736200-b789-11e8-9c81-9b95bee7d962.png" alt="image"> 并使用 <strong>天然平滑版本的共生分布</strong> <img src="https://user-images.githubusercontent.com/11768073/45485339-d9f64c80-b789-11e8-883b-deb50c02a7cf.png" alt="image"> 来获得稍好的结果。 平滑也简化了软件开发，因为分布平滑分布的KL-发散是自动有限的。</p><p>由于我们在没有类别的先验知识的情况下没有意识到关于词库提取的其他研究，我们将 <img src="https://user-images.githubusercontent.com/11768073/45485306-ae736200-b789-11e8-9c81-9b95bee7d962.png" alt="image"> 排名与来自[15]的权重加权的歧视值和一些更简单和天真的排名进行了比较。</p><p>为了计算判别值（DV），我们使用 <strong>余弦相异性</strong> 实现了[18]中的算法。 对于我们的语料库，我们发现这种方法强烈支持特定于一个或几个文档的术语，尤其是大文档。 此方法可能适合查找特定文档的区分术语，但似乎不太适合识别有用的全局关键字。</p><p>我们评估的第一个“天真”方法是采用最常用的词。 这种方法给出了合理的结果，可能是因为我们只考虑名词，动词（没有辅助词）和专有名称。最后一种方法是计算每个术语的所有文档的平均 tf.idf 值。我们使用以下公式来计算语料库 C，术语 t 和文档的 tf.idf <img src="https://user-images.githubusercontent.com/11768073/45485888-bcc27d80-b78b-11e8-84ec-bbfe1fb83182.png" alt="image"> ，其中 M 是 C 中的文档数，<img src="https://user-images.githubusercontent.com/11768073/45485917-dd8ad300-b78b-11e8-9cd7-7470a791f6b6.png" alt="image"> 其中 <img src="https://user-images.githubusercontent.com/11768073/45485933-eda2b280-b78b-11e8-9df2-c83706e1e80b.png" alt="image"></p><h3 id="5-3-结果"><a href="#5-3-结果" class="headerlink" title="5.3 结果"></a>5.3 结果</h3><p>为了评估生成的同义词库，我们确定（a）真正提供信息的词库术语的数量，以及（b）所涵盖的数据集中的主题数量。 <strong>这两个评估方面类似于通常的精确度和召回措施</strong>。</p><p>为了评估精确度方面（a），我们将生成的词库术语与荷兰声音和视觉研究所使用的关键词词典进行了比较， the Gemeenschappelijke Thesaurus voor Audiovisuele Archieven（GTAA，视听档案的通用词库），包含约9000个主题词汇和广泛的人名，公司名称和名单地名[12]。 在该词库中找到的关键词部分用作指示自动生成的词库术语集的质量。</p><p>召回方面（b）预先假定对语料库中的主题有所了解。 当然，完整的“主题”列表有点争论，但由于我们从八个维基百科类别中选择了文章，因此将每个维基百科类别统计为一个单独的主题似乎是合理的。 为了了解这些主题的涵盖范围，我们为每个类别提取了五个最佳关键字。 从而我们获得了一组30个术语。如果在没有对这些类别进行培训的情况下找到更多这些术语，则同义词生成算法被认为是更好的。为了找到每个类别的最佳关键词，我们使用了[1]提出的方法的略微变化：我们将整个语料库的术语分布与子集的术语分布进行比较。选择那些具有最大偏差分布的术语。 结果组的关键字在表2中给出。</p><p><img src="https://user-images.githubusercontent.com/11768073/45486353-5b031300-b78d-11e8-8476-56fb5a0e05ed.png" alt="image"></p><p>GTAA对最佳n项的覆盖率的结果如表3（左）所示。 我们已经确定 n 的覆盖范围是 40,80 和 160 。低百分比部分是由于许多生成的词库术语不会被认为是好的关键词（既不直观也不在GTAA中），尽管它们确实与特定域明确相关，例如 ‘guitar player ‘ 或 ‘French’。对于<img src="https://user-images.githubusercontent.com/11768073/45486436-ab7a7080-b78d-11e8-8c18-cd9dcdb8f387.png" alt="image">方法，我们看到略微倾向于给予更有意义的术语更高级别。 <strong>我们的方法的结果明显优于平均 tf.idf，并且在低水平上略好于术语频率方法</strong>。</p><p><img src="https://user-images.githubusercontent.com/11768073/45486414-956cb000-b78d-11e8-845d-3ccfea760b85.png" alt="image"></p><p>为了评估从中获取文本的八个主题的召回，我们可以计算表 2 中的关键字的数量或者在建议的列表中存在关键字（根据相同的表）的类别的数量。表3（右）中给出了集合 40，80和160的最佳术语每种方法对应情况。<strong>令人惊讶的是，简单的术语频率为此评估指标提供了最佳结果</strong>。在低水平，本文提出的方法明显优于平均tf.idf。在仔细检查生成的术语时，有些令人失望的结果的一个原因立即变得清晰：我们的方法倾向于选择具有许多具有相似分布的共同词的词。这意味着也可以找到所有这些共同发生的词。因此，大多数排名最高的术语是近似同义词，或者至少是同一主题上的术语，从而抑制不太突出的主题。具有相似分布的项的聚类（例如，使用对称 KL-divergence 作为相似性度量）将是该问题的解决方案。但是，结果很大程度上取决于聚类方法，很难与其他方法进行比较。</p><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6 总结"></a>6 总结</h2><p>我们开发了一个信息理论度量 <img src="https://user-images.githubusercontent.com/11768073/45486591-41160000-b78e-11e8-8d8f-3bda3626462c.png" alt="image"> ，根据术语与其他术语的共同出现，将术语的有用性作为关键词进行排名。 该度量在术语的特定性和一般性之间给出了自然平衡，并且检测可以用术语表征的文档集合的大且连贯的子集。 为了生成给定语料库的同义词库，我们在一些过滤后使用排名最高的术语。 为了评估这个衡量标准，我们用荷兰语维基百科的文章构建了一个语料库，同样取自 8 个类别。 作为精确度的衡量标准，我们已经确定了仔细包含的 n 个排名最高的术语的分数构建关键词的一般词库。作为召回的近似值，我们将 n 个排名最高的术语与用 8 个原始类别中的每个类别的独立方法提取的关键词集进行了比较。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>我们感谢 Wolf Huijsen 从维基百科中提取数据。 本文的研究是荷兰政府根据合同 BSIK 03031赞助的MultimediaN 项目的一部分。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] M. A. Andrade and A. Valencia. Automatic extraction of keywords from scientific text: application to<br>the knowledge domain of protein families. Bioinformatics, 14(7):600–607, 1998.<br>[2] T. Cover and J. Thomas. Elements of information theory. John Wiley and Sons, Inc., 1991.<br>[3] C. J. Crouch and B. Yang. Experiments in automatic statistical thesaurus construction. In SIGIR ’92:<br>Proceedings of the 15th annual international ACM SIGIR conference on Research and development in<br>information retrieval, pages 77–88, New York, NY, USA, 1992. ACM.<br>[4] H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. GATE: A framework and graphical de-<br>velopment environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary<br>Meeting of the Association for Computational Linguistics, 2002.<br>[5] S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deerwester, and R. Harshman. Using latent semantic<br>analysis to improve access to textual information. In CHI ’88: Proceedings of the SIGCHI conference<br>on Human factors in computing systems, pages 281–285, New York, NY, USA, 1988. ACM.<br>[6] T. Hofmann. Probabilistic latent semantic analysis. In UAI99: Uncertainty in artificial intelligence,<br>1999.[7] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning,<br>42(1-2):177–196, January 2001.<br>[8] A. Hulth and B. Megyesi. A study on automatically extracted keywords in text categorization. In ACL.<br>The Association for Computer Linguistics, 2006.<br>[9] T. Landauer, P. Foltz, and D. Laham. Introduction to latent semantic analysis. Discourse Processes,<br>25:259–284, 1998.<br>[10] H.LiandK.Yamanishi. Topicanalysisusingafinitemixturemodel. Inf.Process.Manage., 39(4):521–<br>541, 2003.<br>[11] K. Lund and C. Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence.<br>Behaviour Research Methods, Instruments, &amp; Computers, 28(2):203–208, 1996.<br>[12] V. Malaisé, L. Gazendam, and H. Brugman. Disambiguating automatic semantic annotation based<br>on a thesaurus structure. In Actes de la 14e conférence sur le Traitement Automatique des Langues<br>Naturelles, pages 197–206, 2007.<br>[13] C. D. Manning and H. Schütze. Foundations of Statistical Natural Language Processing. The MIT<br>Press, Cambridge, Massachusetts, 1999.<br>[14] Y. Niwa and Y. Nitta. Co-occurrence vectors from corpora vs. distance vectors from dictionaries. In<br>Proceedings of the 15th conference on Computational linguistics, pages 304–309, Morristown, NJ,<br>USA, 1994. Association for Computational Linguistics.<br>[15] G. Salton, C. Yang, and C. Yu. A theory of term importannce in automatic text analysis. Technical<br>Report TR 74-208, Department of Computer Science, Cornell University, July 1974.<br>[16] H. Schmid. Probabilistic part-of-speech tagging using decision trees. In International Conference on<br>New Methods in Language Processing, Manchester, UK, 1994. unknown.<br>[17] P. K. Shah, C. Perez-Iratxeta, P. Bork, and M. A. Andrade. Information extraction from full text<br>scientific articles: Where are the keywords? BMC Bioinformatics, 4(20):1–9, 2003.<br>[18] P. Willett. An algorithm for the calculation of exact term discrimination values. Inf. Process. Manage.,<br>21(3):225–232, 1985.</p><div class="post-announce">感谢您的阅读，本文由 <a href="https://github.com/zhililab">ZHILI</a> 版权所有。如若转载，请注明出处：ZHILI（<a href="https://github.com/zhililab/2018/09/17/Paper2/">https://github.com/zhililab/2018/09/17/Paper2/</a>）</div><div class="post__prevs"><div class="post__prev"><a href="/2018/06/08/Vulnerability-Chrome-V8/" title="Chrome V8 引擎严重漏洞"><i class="iconfont icon-prev"></i>Chrome V8 引擎严重漏洞</a></div><div class="post__prev post__prev--right"><a href="/2018/09/17/Paper1/" title="论文：基于 Amharic WordNet 的语义网络自动构建研究">论文：基于 Amharic WordNet 的语义网络自动构建研究<i class="iconfont icon-next"></i></a></div></div></div></article><div id="comment-container"></div></div><aside class="page__sidebar"><form id="page-search-from" class="page__search-from" action="/search/"><label class="search-form__item"><input class="input" type="text" name="search" placeholder="Search..."> <i class="iconfont icon-search"></i></label></form><div class="sidebar__block"><h3 class="block__title">简介</h3><p class="block__text">介绍：日常学习与技术交流的个人博客</p></div><div class="sidebar__block"><h3 class="block__title">文章分类</h3><ul class="block-list"><li class="block-list-item"><a class="block-list-link" href="/categories/随笔/">随笔</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/读书/">读书</a><span class="block-list-count">3</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/考试/">考试</a><span class="block-list-count">1</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/技术/">技术</a><span class="block-list-count">5</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/安全/">安全</a><span class="block-list-count">6</span></li><li class="block-list-item"><a class="block-list-link" href="/categories/NLP/">NLP</a><span class="block-list-count">2</span></li></ul></div><div class="sidebar__block"><h3 class="block__title">最新文章</h3><ul class="block-list latest-post-list"><li class="latest-post-item"><a href="/2018/09/17/Paper1/" title="论文：基于 Amharic WordNet 的语义网络自动构建研究"><div class="item__cover"><img src="/assets/images/cover/Paper_cover.png" alt="论文：基于 Amharic WordNet 的语义网络自动构建研究"></div><div class="item__info"><h3 class="item__title">论文：基于 Amharic WordNet 的语义网络自动构建研究</h3><span class="item__text">2018-09-17</span></div></a></li><li class="latest-post-item"><a href="/2018/09/17/Paper2/" title="论文：基于 Co-occurrence 的词库自动生成研究"><div class="item__cover"><img src="/assets/images/cover/Paper_cover.png" alt="论文：基于 Co-occurrence 的词库自动生成研究"></div><div class="item__info"><h3 class="item__title">论文：基于 Co-occurrence 的词库自动生成研究</h3><span class="item__text">2018-09-17</span></div></a></li><li class="latest-post-item"><a href="/2018/06/08/Vulnerability-Chrome-V8/" title="Chrome V8 引擎严重漏洞"><div class="item__cover"><img src="/assets/images/cover/Vulnerability_ChromeV8_cover.png" alt="Chrome V8 引擎严重漏洞"></div><div class="item__info"><h3 class="item__title">Chrome V8 引擎严重漏洞</h3><span class="item__text">2018-06-08</span></div></a></li><li class="latest-post-item"><a href="/2018/06/01/Reading-Books-April-to-May-2018/" title="书单：2018年4月-5月"><div class="item__cover"><img src="/assets/images/cover/Reading-Books-Feb-2018_cover.jpg" alt="书单：2018年4月-5月"></div><div class="item__info"><h3 class="item__title">书单：2018年4月-5月</h3><span class="item__text">2018-06-01</span></div></a></li></ul></div><div class="sidebar__block"><h3 class="block__title">文章标签</h3><ul class="block-list tag-list clearfix"><li class="tag-item"><a class="tag-link" href="/tags/Git/">Git</a></li><li class="tag-item"><a class="tag-link" href="/tags/Kali-Linux/">Kali Linux</a></li><li class="tag-item"><a class="tag-link" href="/tags/Meetup/">Meetup</a></li><li class="tag-item"><a class="tag-link" href="/tags/OWASP/">OWASP</a></li><li class="tag-item"><a class="tag-link" href="/tags/RSA/">RSA</a></li><li class="tag-item"><a class="tag-link" href="/tags/TOEFL/">TOEFL</a></li><li class="tag-item"><a class="tag-link" href="/tags/WordNet/">WordNet</a></li><li class="tag-item"><a class="tag-link" href="/tags/威胁情报/">威胁情报</a></li><li class="tag-item"><a class="tag-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-item"><a class="tag-link" href="/tags/漏洞/">漏洞</a></li><li class="tag-item"><a class="tag-link" href="/tags/英语/">英语</a></li><li class="tag-item"><a class="tag-link" href="/tags/词库自动构建/">词库自动构建</a></li><li class="tag-item"><a class="tag-link" href="/tags/词汇共现/">词汇共现</a></li><li class="tag-item"><a class="tag-link" href="/tags/语义网络/">语义网络</a></li><li class="tag-item"><a class="tag-link" href="/tags/读书/">读书</a></li><li class="tag-item"><a class="tag-link" href="/tags/随笔/">随笔</a></li></ul></div></aside></main><footer class="page__footer"><section class="footer__top"><div class="page__container footer__container"><div class="footer-top__item footer-top__item--2"><h3 class="item__title">关于</h3><div class="item__content"><p class="item__text">本站是基于 Hexo 搭建的静态资源博客，主要用于分享技术总结、随笔及考试的一些心得总结，欢迎点击右下角订阅 rss。</p><ul class="footer__contact-info"><li class="contact-info__item"><i class="iconfont icon-address"></i> <span>Nanjing, Jiangsu Province, China</span></li><li class="contact-info__item"><i class="iconfont icon-email2"></i> <span>lz123321@live.com</span></li></ul></div></div><div class="footer-top__item footer__image"><img src="/assets/images/avatar/me.jpg" alt="logo" title="ZHILI"></div><div class="footer-top__item"><h3 class="item__title">友情链接</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="http://e.huawei.com/cn/products/enterprise-networking/security" title="华为企业安全" target="_blank">华为企业安全</a></li><li class="list-item"><a href="https://s.tencent.com/" title="腾讯企业安全" target="_blank">腾讯企业安全</a></li><li class="list-item"><a href="https://jaq.alibaba.com/" title="阿里聚安全" target="_blank">阿里聚安全</a></li><li class="list-item"><a href="http://b.360.cn/" title="360企业安全" target="_blank">360企业安全</a></li><li class="list-item"><a href="https://www.shentoushi.top/" title="渗透师导航" target="_blank">渗透师导航</a></li><li class="list-item"><a href="https://www.freebuf.com/" title="FreeBuff" target="_blank">FreeBuff</a></li><li class="list-item"><a href="https://bbs.pediy.com/" title="看雪安全论坛" target="_blank">看雪安全论坛</a></li></ul></div></div><div class="footer-top__item"><h3 class="item__title">构建工具</h3><div class="item__content"><ul class="footer-top__list"><li class="list-item"><a href="https://pages.github.com/" title="GitHub Pages" target="_blank">GitHub Pages</a></li><li class="list-item"><a href="https://hexo.io/" title="Blog Framework" target="_blank">Hexo</a></li></ul></div></div></div></section><section class="footer__bottom"><div class="page__container footer__container"><p class="footer__copyright">© All rights reserved 2018 <a href="https://www.zhililab.com/" target="_blank">Zhi Li Lab</a></p><ul class="footer__social-network clearfix"><li class="social-network__item"><a href="https://github.com/zhililab" target="_blank" title="github"><i class="iconfont icon-github"></i></a></li><li class="social-network__item"><a href="https://www.linkedin.com/in/zhili1993" target="_blank" title="linkedin"><i class="iconfont icon-in"></i></a></li><li class="social-network__item"><a href="http://weibo.com/cnzhili" target="_blank" title="weibo"><i class="iconfont icon-weibo"></i></a></li><li class="social-network__item"><a href="mailto:lz123321@live.com" target="_blank" title="email"><i class="iconfont icon-email"></i></a></li><li class="social-network__item"><a href="/atom.xml" target="_blank" title="rss"><i class="iconfont icon-rss"></i></a></li></ul></div></section></footer><div id="back-top" class="back-top back-top--hidden js-hidden"><i class="iconfont icon-top"></i></div></div><script src="/js/common.js"></script><script src="/js/page/post.js"></script><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="/js/md5.min.js"></script><script>var tags=["词库自动构建","词汇共现"],gitalk=new Gitalk({clientID:"",clientSecret:"453b1f30b04c679d76496ec98dcc95ffd9c29679",repo:"zhililab.github.io",owner:"zhililab",admin:["zhililab"],labels:tags,id:new Date(15371136e5).getTime()>new Date("2018-02-15").getTime()?md5(location.href):location.href});gitalk.render("comment-container")</script><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>